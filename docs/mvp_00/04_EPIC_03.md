# Epic 3: OpenAI Responses Client

## Functional Analysis & Design
- Provide an async client wrapper around the OpenAI Responses API that sends a turn (messages + tools + model options) and streams deltas back to the conversation manager/TUI with tool-call callbacks.
- Accept session-level options (model id, reasoning effort), advertised tools (JSON + freeform apply_patch), and message history; surface streamed text, tool call starts, tool call deltas, and completions.
- Expose clear error semantics for auth failures, timeouts, rate limits, and transport errors; return user-friendly descriptions suitable for UI/log logging.
- Offer pluggable transports: real HTTP (httpx) and an in-memory/mock transport for deterministic tests and offline runs.
- Provide flow-control/back-pressure friendly streaming so downstream consumers (TUI) can keep up without unbounded buffering.

## Technical Analysis & Design
- **API surface (`OpenAIResponsesClient`)**
  - `async submit(request: ConversationRequest) -> AsyncIterator[ResponseEvent]` yields structured events: `TextDelta`, `ToolCallStart`, `ToolCallDelta`, `ToolCallEnd`, `MessageDone`, `Error`.
  - `ConversationRequest` fields: `messages` (history), `model`, `reasoning_effort`, `tools` (advertised JSON schema + freeform apply_patch entry), optional `max_output_tokens`, `metadata` (trace/session ids), `timeout`.
  - Callers pass callbacks by consuming the async iterator; no hidden threads.
- **Domain models (`lincona/openai_client/types.py`)**
  - `Message` (role, content, tool_call_id optional), `ToolDefinition` (name, description, parameters), `ApplyPatchFreeform` marker for freeform tool.
  - Response-side types mirroring OpenAI Responses schema: `ToolCallPayload` (id, name, arguments JSON string), `DeltaChunk` (text/tool deltas), `ResponseEvent` union.
  - Typed error classes: `ApiAuthError`, `ApiRateLimitError`, `ApiTimeoutError`, `ApiServerError`, `ApiClientError`, `StreamingParseError`.
- **Transport abstraction**
  - `ResponsesTransport` protocol with `async stream_response(payload: dict[str, Any]) -> AsyncIterator[bytes | str]`.
  - Implementations:
    - `HttpResponsesTransport`: httpx.AsyncClient with default timeouts (connect=5s, read=60s), **single-attempt** request strategy (no automatic retries) to surface failures immediately; configurable base_url (default `https://api.openai.com/v1`).
    - `MockResponsesTransport`: seeded with predetermined events/chunks; used in unit tests and offline mode.
- **Request assembly**
  - Build payload matching OpenAI Responses endpoint: `model`, `messages`, `tools` (list of JSON tool specs plus freeform entry), `metadata` (session_id, trace_id), `reasoning`/`reasoning_effort` passthrough, `stream=true`.
  - Inject `Authorization: Bearer <api_key>` and `OpenAI-Beta` headers if required; include `User-Agent` with app/version.
  - Validate inputs before network call (non-empty model, api_key present unless using mock).
- **Streaming & parsing**
  - Use server-sent style chunks (lines beginning with `data:`) parsed incrementally; ignore heartbeat/empty lines.
  - Each parsed JSON chunk mapped to `ResponseEvent` variants; recover tool call lifecycle by buffering `tool_call_id` and accumulating arguments/text.
  - Enforce max buffered tokens per tool call to prevent unbounded memory (configurable, defaults ~32KB); emit `StreamingParseError` on malformed chunks.
- **Flow control**
  - Streaming exposed as async generator; downstream can `aiter` at its own pace. Optional bounded `asyncio.Queue` wrapper (`consume_stream(queue_max=128)`) that back-pressures `aiter_send`.
  - Cancellation handling: if consumer cancels, propagate to close HTTP stream cleanly.
- **Error handling & retries**
  - Pre-flight 401/403 -> `ApiAuthError`; 429 -> `ApiRateLimitError` with retry-after parsed; ≥500 -> `ApiServerError`; network timeouts -> `ApiTimeoutError`.
  - Translate errors into `Error` events plus exceptions; caller chooses whether to raise or render friendly message.
  - Logging hooks: optional logger injected; logs request id, status, latency.
- **Configuration hooks**
  - Pull defaults from `Settings` (model, reasoning_effort); api_key from settings/env.
  - Allow overriding base_url and timeout via constructor (useful for mock servers/E2E).
  - Allow injecting transport (mock vs HTTP) via constructor or flag to support offline/unit/E2E modes.
- **Testing strategy**
  - Unit tests use `MockResponsesTransport` to simulate text-only, tool-call, and error streams.
  - Contract tests with httpx MockTransport to verify payload shape and header injection.
  - Cancellation/back-pressure tests to ensure queues do not drop events and cleanly close.

## Architecture
- New package `lincona/openai_client/`:
  - `client.py`: `OpenAIResponsesClient` orchestrator using injected `ResponsesTransport`.
  - `transport.py`: `ResponsesTransport` protocol, `HttpResponsesTransport`, `MockResponsesTransport`.
  - `types.py`: request/response dataclasses, enums, error classes, event variants.
  - `parsing.py`: streaming chunk parser to convert raw lines → `ResponseEvent`.
- Integration points:
  - Conversation/session manager (Epic 5) consumes `OpenAIResponsesClient.submit(...)` iterator and forwards events to TUI/logging.
  - Config module supplies defaults (model, reasoning_effort, api_key); persistence/logging capture request ids and errors.

## Implementation Steps (sequential checklist)
[x] Add `lincona/openai_client/types.py` with request/response/event dataclasses and error enums/exceptions.
[ ] Add `lincona/openai_client/transport.py` defining `ResponsesTransport`, `HttpResponsesTransport` (httpx AsyncClient, headers, timeouts, single-attempt), and `MockResponsesTransport`.
[ ] Add `lincona/openai_client/parsing.py` to parse streamed chunks (SSE-style `data:` lines) into `ResponseEvent` variants with tool-call lifecycle handling and size guards.
[ ] Implement `OpenAIResponsesClient` in `lincona/openai_client/client.py` to assemble payloads, invoke transport, yield structured events, and map HTTP errors to domain errors.
[ ] Wire configuration defaults (model, reasoning_effort, api_key) into client constructor; allow base_url/timeout overrides for tests/E2E.
[ ] Add unit tests covering payload assembly, streaming text/tool calls, error mapping, cancellation/back-pressure, and mock transport behavior.
[ ] Document usage (README or dev notes) and tick Epic 3 in `docs/mvp_00/01_PLAN.md` after completion.

## Q&A Log
- Q1: Preferred default retry policy for Responses API (initial request only)? **Answer:** b) Single attempt, surface error immediately.
- Q2: Max buffered size per tool call before emitting `ToolCallEnd`? **Answer:** a) 32KB default, but keep configurable.
